{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "428516ee",
   "metadata": {},
   "source": [
    "# This Notebook looks at Titanic Survival Data using SparkML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "12cff1ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from pprint import pprint\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from timeit import default_timer as timer\n",
    "\n",
    "from kdp_connector import KdpConn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c27a0ede",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://towardsdatascience.com/predicting-the-survival-of-titanic-passengers-30870ccc7e8\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "23f54a62",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pyspark'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/b0/q6bjxglj5mnfj037xbwjjq2m0000gn/T/ipykernel_15808/2611060278.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m#import findspark\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m#findspark.init()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSparkSession\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pyspark'"
     ]
    }
   ],
   "source": [
    "#find pyspark installation\n",
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "#spark imports\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.feature import IndexToString, StringIndexer, VectorIndexer,VectorAssembler\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2ab4820b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#access token from oauth\n",
    "jwt = os.getenv('ACCESS_TOKEN')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db57280f",
   "metadata": {},
   "source": [
    "## First Connect to Koverse"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "271917d3",
   "metadata": {},
   "source": [
    "## Then retrieve data into Spark DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6760308f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This example shows you how to use the KDP Python Connector to read data from KDP dataset into\n",
    "# a Pandas Dataframe..\n",
    "\n",
    "# get dataset_id from url in kdp4 when dataset is selected\n",
    "################## Replace with your INFO #####################\n",
    "email = 'spongebob@koverse.com'\n",
    "password = 'Password1!'\n",
    "workspace_id = 'spongebob'\n",
    "dataset_id = '12345678-1234-asd1-fgh2-378b59bf74ce'\n",
    "###############################################################\n",
    "\n",
    "\n",
    "host = 'https://api.dev.koverse.com'\n",
    "batch_size = 100000\n",
    "starting_record_id = ''\n",
    "path_to_ca_file = ''\n",
    "\n",
    "\n",
    "\n",
    "kdp_conn = KdpConn(path_to_ca_file=path_to_ca_file, host=host)\n",
    "#jwt = kdp_conn.create_authentication_token(email=email,password=password,workspace_id=workspace_id)\n",
    "pDF = kdp_conn.read_dataset_to_pandas_dataframe(dataset_id=dataset_id,\n",
    "                                                      jwt=jwt,\n",
    "                                                      starting_record_id=starting_record_id,\n",
    "                                                      batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28c55c55",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create PySpark SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .master(\"local[1]\") \\\n",
    "    .appName(\"temp1\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02446113",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=spark.createDataFrame(pDF) \n",
    "df.printSchema()\n",
    "df.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0dd7a3c",
   "metadata": {},
   "source": [
    "## Explore with SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8af6fc67",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.createOrReplaceTempView(\"titanicTemp\")\n",
    "result = spark.sql('''\n",
    "   SELECT sex, AVG(age) as avg_age\n",
    "   FROM titanicTemp\n",
    "   GROUP BY sex\n",
    "   ''')\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40e7182f",
   "metadata": {},
   "source": [
    "## Explore data using a dataframe or using Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eff9e5a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "pDF.info(verbose=True)\n",
    "pDF.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "098a52e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "pDF[\"Survived\"] = pd.to_numeric(pDF[\"Survived\"])\n",
    "pDF[\"Age\"] = pd.to_numeric(pDF[\"Age\"])\n",
    "pDF[\"Sex\"] = pDF[\"Sex\"].astype(str)\n",
    "survived = 'Survived'\n",
    "not_survived = 'not survived'\n",
    "fig, axes = plt.subplots(nrows=1, ncols=2,figsize=(10, 4))\n",
    "women = pDF[pDF['Sex']=='female']\n",
    "men = pDF[pDF['Sex']=='male']\n",
    "ax = sns.distplot(women[women['Survived']==1].Age.dropna(), bins=18, label = survived, ax = axes[0], kde =False)\n",
    "ax = sns.distplot(women[women['Survived']==0].Age.dropna(), bins=18, label = not_survived, ax = axes[0], kde =False)\n",
    "ax.legend()\n",
    "ax.set_title('Female')\n",
    "ax = sns.distplot(men[men['Survived']==1].Age.dropna(), bins=18, label = survived, ax = axes[1], kde = False)\n",
    "ax = sns.distplot(men[men['Survived']==0].Age.dropna(), bins=18, label = not_survived, ax = axes[1], kde = False)\n",
    "ax.legend()\n",
    "ax = ax.set_title('Male')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e57b6b09",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = sns.FacetGrid(pDF, col='Survived', row='Pclass', height=2.2, aspect=1.6)\n",
    "grid.map(plt.hist, 'Age', alpha=.5, bins=20)\n",
    "grid.add_legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42fb0373",
   "metadata": {},
   "source": [
    "## Create Features to be used in model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6cd10a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = int(pDF[\"Age\"].mean())\n",
    "print(mean,type(mean))\n",
    "\n",
    "df2 = df.withColumn(\"male\", (F.when(F.col(\"Sex\") == 'male', 1).otherwise(0)))\n",
    "df2 = df2.withColumn(\"Age\",df2.Age.cast('int'))\n",
    "df2 = df2.withColumn(\"embarked_num\", (F.when(F.col(\"Embarked\") == 'Q', 1).otherwise(0)))\n",
    "df2 = df2.withColumn(\"age_new\", (F.when(F.col(\"Age\").isNull() | F.isnan(F.col(\"Age\")), mean).otherwise(F.col(\"Age\").cast('int'))))\n",
    "df2 = df2.withColumn(\"Parch\",df2.Parch.cast('int'))\n",
    "df2 = df2.withColumn(\"Pclass\",df2.Pclass.cast('int'))\n",
    "df2 = df2.withColumn(\"SibSp\",df2.SibSp.cast('int'))\n",
    "df2 = df2.withColumn(\"Survived\",df2.Survived.cast('int'))\n",
    "df2 = df2.withColumn(\"embarked_num\",df2.embarked_num.cast('int'))\n",
    "df2 = df2.withColumn(\"male\",df2.male.cast('int'))\n",
    "df2 = df2.withColumn(\"age_new\",df2.age_new.cast('int'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35c0d800",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.toPandas()[['age_new','embarked_num', 'male', 'Parch', 'Pclass', 'SibSp']].info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a5bce7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['age_new','embarked_num', 'male', 'Parch', 'Pclass', 'SibSp']\n",
    "\n",
    "\n",
    "vecassemb = VectorAssembler(inputCols=cols,outputCol='features')\n",
    "df3 = vecassemb.transform(df2)\n",
    "\n",
    "\n",
    "df_train, df_test = df3.randomSplit([0.8,0.2])\n",
    "\n",
    "rf = RandomForestClassifier(featuresCol='features',labelCol='Survived')\n",
    "model = rf.fit(df_train)\n",
    "result = model.transform(df_test)\n",
    "\n",
    "\n",
    "predictionAndLabels = result.select(\"prediction\", \"Survived\")\n",
    "\n",
    "# Select (prediction, true label) and compute test error\n",
    "evaluator = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"Survived\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "accuracy = evaluator.evaluate(result)\n",
    "print(\"Test Error = %g\" % (1.0 - accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1682e6a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_results = model.transform(df3)\n",
    "all_results.select('name','survived','prediction').toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e29a455a",
   "metadata": {},
   "outputs": [],
   "source": [
    "importances = model.featureImportances\n",
    "\n",
    "x_values = list(range(len(importances)))\n",
    "\n",
    "plt.bar(x_values, importances, orientation = 'vertical')\n",
    "plt.xticks(x_values, cols, rotation=40)\n",
    "plt.ylabel('Importance')\n",
    "plt.xlabel('Feature')\n",
    "plt.title('Feature Importances')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36d65824",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d027026",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfOut = all_results.toPandas()[['age_new','embarked_num', 'male', 'Parch', 'Pclass', 'SibSp','Name','Survived','prediction']]\n",
    "dfOut.Name = dfOut.Name.astype('string')\n",
    "dfOut.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62d295f0",
   "metadata": {},
   "source": [
    "## Save back to Koverse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "819ecf8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## ingest data - replace dataset_id param with your own and uncomment below to save back to koverse\n",
    "\n",
    "#partitions_set = kdp_conn.ingest(dfOut, \"5ffbfb5b-6394-4d60-836f-ef99fc582d06\", jwt, batch_size)\n",
    "#pprint('partitions: %s' % partitions_set)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
