{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "659bdacd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from kdp_connector import KdpConn\n",
    "import pandas as pd\n",
    "import kdp_api\n",
    "from kdp_api.api import write_api\n",
    "from kdp_api.api import datasets_api\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from dateutil.relativedelta import relativedelta\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d51a229e",
   "metadata": {},
   "source": [
    "# This notebook will go over how to:\n",
    "## 1. Write data into KDP (a. As a new dataset b. Appending to an existing dataset c. Overwriting an existing dataset) from a manual upload or possibly from an automatic process\n",
    "## 2. Read data from KDP\n",
    "## 3. Perform 1 and 2 in a flow (forward or backward), making it possible to create dataflows that are connected to external data sources\n",
    "## using Pandas dataframes. There will be a separate notebook showing Pyspark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bd75097",
   "metadata": {},
   "source": [
    "### Define a few helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "8055de32",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dfs_equivalent(df1, df2):\n",
    "    \n",
    "    '''\n",
    "    Function to help determine if two dataframe are equivalent. Equivalent meaning that they share the same\n",
    "    data except the order of rows or columns differ\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    \n",
    "    #Number of duplicated rows\n",
    "    df1_dupe = df1.duplicated().sum()\n",
    "    df2_dupe = df2.duplicated().sum()\n",
    "    \n",
    "    if df1_dupe != df2_dupe:\n",
    "        return False\n",
    "\n",
    "    DF1 = df1.copy().drop_duplicates()\n",
    "    DF2 = df2.copy().drop_duplicates()\n",
    "\n",
    "    mergeData = pd.merge(DF1, DF2, on = list(DF2.columns), how = 'inner')\n",
    "\n",
    "    if len(DF1) == len(DF2) == len(mergeData):\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "5ccf3bea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dfs_similar_data(df1, df2):\n",
    "    \n",
    "    '''\n",
    "    Function to help determine if two dataframes are similar. Similar meaning that they have the same column\n",
    "    names and data types for those columns.\n",
    "    '''\n",
    "    \n",
    "    df1_dataTypes = pd.DataFrame(df1.dtypes).reset_index()\n",
    "    df1_dataTypes.columns = ['Column', 'Type']\n",
    "    df2_dataTypes = pd.DataFrame(df2.dtypes).reset_index()\n",
    "    df2_dataTypes.columns = ['Column', 'Type']\n",
    "    \n",
    "    mergeData = pd.merge(df1_dataTypes, df2_dataTypes, on = ['Column', 'Type'], how = 'inner')\n",
    "    \n",
    "    if len(df1_dataTypes) == len(df2_dataTypes) == len(mergeData):\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "97bcbaa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dfs_difference_types(df1, df2):\n",
    "    '''\n",
    "    Helper function used to help output the differences between two input dataframes when they are not the\n",
    "    same.\n",
    "    '''\n",
    "    \n",
    "    df1_dataTypes = pd.DataFrame(df1.dtypes).reset_index()\n",
    "    df1_dataTypes.columns = ['Column', 'Type']\n",
    "    df2_dataTypes = pd.DataFrame(df2.dtypes).reset_index()\n",
    "    df2_dataTypes.columns = ['Column', 'Type']\n",
    "    \n",
    "    mergeData = pd.merge(df1_dataTypes, df2_dataTypes, on = ['Column', 'Type'], how = 'outer', indicator = True)\n",
    "    mergeData = mergeData[mergeData['_merge'].isin(['left_only', 'right_only'])]\n",
    "    \n",
    "    return mergeData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "44a738f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def overwrite_to_kdp(data, dataset_id, workspace_id, jwt, batch_size, starting_record_id, equivalenceCheck):\n",
    "    '''\n",
    "    This function provides a way to more directly write over an existing dataset name by deleting the existing\n",
    "    dataset and replacing it with the transformed version. \n",
    "    \n",
    "    This feature likely should be used for normalizations as with transformations are typically written to another\n",
    "    dataset.\n",
    "    \n",
    "    Ideally, there would be a way to define a custom dataset ID in order to assign the new dataset to have the\n",
    "    same dataset ID as the original so that any pointers to the dataset ID are not disrupted. Slight modifications\n",
    "    would be required to accomodate for the change since two datasets can't have the same ID. A temporary dataset\n",
    "    ID would need to be created first to ensure the ingest is working properly before deleting the original dataset\n",
    "    then creating the replacement dataset with the original dataset ID.\n",
    "    \n",
    "    Parameters:\n",
    "    \n",
    "    data - pandas df to write to KDP, the one that was read and normalized/transformed.\n",
    "    dataset_id - the dataset ID of this of the dataset originally read into Jupyter\n",
    "    workspace_id - matches initial settings\n",
    "    jwt - matches initial settings\n",
    "    batch_size - matches initial settings\n",
    "    starting_record_id - matches initial settings\n",
    "    equivalenceCheck - boolean option to check if input dataframe and reading from KDP are equivalent (having\n",
    "    the same rows except in a different order).\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    ingestFailed = False\n",
    "    \n",
    "    #API Config\n",
    "    configuration = kdp_api.Configuration(\n",
    "        host='https://api.dev.koverse.com'\n",
    "    )\n",
    "    configuration.access_token = jwt\n",
    "\n",
    "    \n",
    "    #Get current dataset name\n",
    "    current_name = kdp_conn.get_dataset(dataset_id=dataset_id, jwt=jwt).name\n",
    "    \n",
    "    #Create new dataset with same name on KDP\n",
    "    dataset = kdp_conn.create_dataset(name=current_name, workspace_id=workspace_id, jwt=jwt)\n",
    "    new_dataset_id = dataset.id\n",
    "    \n",
    "    #Ingest data into newly created dataset\n",
    "    try:\n",
    "        partitions_set = kdp_conn.ingest(data, new_dataset_id, jwt, batch_size)\n",
    "    except:\n",
    "        ingestFailed = True\n",
    "    \n",
    "    if not ingestFailed and equivalenceCheck:\n",
    "        #Read df from KDP\n",
    "        dfCheck = kdp_conn.read_dataset_to_pandas_dataframe(dataset_id=new_dataset_id,\n",
    "                                                  jwt=jwt,\n",
    "                                                  starting_record_id=starting_record_id,\n",
    "                                                  batch_size=batch_size)\n",
    "        if dfs_equivalent(dfCheck, data):\n",
    "            print('equivalenceCheck pass')\n",
    "        else:\n",
    "            print('equivalenceCheck fail:\\n')\n",
    "            print('Input data length: {}\\n'.format(len(data)))\n",
    "            print('Input data non-duplicated length: {}\\n'.format(len(data.drop_duplicates())))\n",
    "            print('KDP data length: {}\\n'.format(len(dfCheck)))\n",
    "            print('KDP data non-duplicated length: {}\\n'.format(len(dfCheck.drop_duplicates)))\n",
    "            print('Merge data length: {}'.format(len(pd.merge(data.drop_duplicates(), dfCheck.drop_duplicates(), \n",
    "                                                              on = list(data.columns), how = 'inner'))))\n",
    "            raise Exception('equivalenceCheck failed, if wish to continue, disable this check.')\n",
    "        \n",
    "             \n",
    "    #API Connect and delete either new dataset or old dataset by id\n",
    "    with kdp_api.ApiClient(configuration) as api_client:\n",
    "        \n",
    "        if ingestFailed:\n",
    "            try:\n",
    "                print('Ingest failed. Attempting to delete newly created dataset')\n",
    "                api_instance = datasets_api.DatasetsApi(api_client)\n",
    "                api_instance.datasets_id_delete(id = new_dataset_id)\n",
    "                print('Newly created dataset {} was successfully deleted.'.format(new_dataset_id))\n",
    "            except kdp_api.ApiException as e:\n",
    "                print(\"Exception : %s\\n\" % e)\n",
    "                raise Exception(\"Error deleting associated dataset id from KDP. See printed error message above.\")\n",
    "            raise Exception('Ingest failed, exiting')\n",
    "            \n",
    "        else:\n",
    "            print('Ingest successful. Deleting old dataset.')\n",
    "            try:\n",
    "                api_instance = datasets_api.DatasetsApi(api_client)\n",
    "                api_instance = datasets_api.DatasetsApi(api_client)\n",
    "                api_instance.datasets_id_delete(id = dataset_id)\n",
    "                print('Dataset {} was successfully deleted.'.format(dataset_id))\n",
    "            except kdp_api.ApiException as e:\n",
    "                print(\"Exception : %s\\n\" % e)\n",
    "                raise Exception(\"Error deleting associated dataset id from KDP. See printed error message above.\")\n",
    "\n",
    "    return new_dataset_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "1667c0db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_to_new_kdp(data, new_dataset_name, workspace_id, jwt, batch_size, starting_record_id, equivalenceCheck):\n",
    "    \n",
    "    '''\n",
    "    This function provides a way to more directly write a dataframe into KDP as a new dataset with Pandas. \n",
    "    Would be utilized for transformations.\n",
    "    \n",
    "    Parameters:\n",
    "    \n",
    "    data - pandas df to write to KDP, the one that was read and normalized/transformed.\n",
    "    dataset_id - the dataset ID of this of the dataset originally read into Jupyter\n",
    "    workspace_id - matches initial settings\n",
    "    jwt - matches initial settings\n",
    "    batch_size - matches initial settings\n",
    "    starting_record_id - matches initial settings\n",
    "    equivalenceCheck - boolean option to check if input dataframe and reading from KDP are equivalent (having\n",
    "    the same rows except in a different order).\n",
    "    \n",
    "    \n",
    "    '''\n",
    "    \n",
    "    \n",
    "    ingestFailed = False\n",
    "    \n",
    "    #Create new dataset on KDP\n",
    "    dataset = kdp_conn.create_dataset(name=new_dataset_name, workspace_id=workspace_id, jwt=jwt)\n",
    "    new_dataset_id = dataset.id\n",
    "\n",
    "    #Ingest data into newly created dataset\n",
    "    try:\n",
    "        partitions_set = kdp_conn.ingest(data, new_dataset_id, jwt, batch_size)\n",
    "    except:\n",
    "        ingestFailed = True\n",
    "            \n",
    "    if not ingestFailed and equivalenceCheck:\n",
    "        #Read df from KDP\n",
    "        dfCheck = kdp_conn.read_dataset_to_pandas_dataframe(dataset_id=new_dataset_id,\n",
    "                                                  jwt=jwt,\n",
    "                                                  starting_record_id=starting_record_id,\n",
    "                                                  batch_size=batch_size)\n",
    "        if dfs_equivalent(dfCheck, data):\n",
    "            print('equivalenceCheck pass')\n",
    "        else:\n",
    "            print('equivalenceCheck fail:\\n')\n",
    "            print('Input data length: {}\\n'.format(len(data)))\n",
    "            print('Input data non-duplicated length: {}\\n'.format(len(data.drop_duplicates())))\n",
    "            print('KDP data length: {}\\n'.format(len(dfCheck)))\n",
    "            print('KDP data non-duplicated length: {}\\n'.format(len(dfCheck.drop_duplicates)))\n",
    "            print('Merge data length: {}'.format(len(pd.merge(data.drop_duplicates(), dfCheck.drop_duplicates(), \n",
    "                                                              on = list(data.columns), how = 'inner'))))\n",
    "            raise Exception('equivalenceCheck failed, if wish to continue, disable this check.')\n",
    "            \n",
    "            \n",
    "    #API Connect and delete new dataset if ingest failed\n",
    "    if ingestFailed:\n",
    "        \n",
    "        #API Config\n",
    "        configuration = kdp_api.Configuration(\n",
    "            host='https://api.dev.koverse.com'\n",
    "        )\n",
    "        configuration.access_token = jwt\n",
    "\n",
    "        \n",
    "        with kdp_api.ApiClient(configuration) as api_client:\n",
    "            try:\n",
    "                print('Ingest failed. Attempting to delete newly created dataset')\n",
    "                api_instance = datasets_api.DatasetsApi(api_client)\n",
    "                api_instance.datasets_id_delete(id = new_dataset_id)\n",
    "                print('Newly created dataset {} was successfully deleted.'.format(new_dataset_id))\n",
    "            except kdp_api.ApiException as e:\n",
    "                print(\"Exception : %s\\n\" % e)\n",
    "                raise Exception(\"Error deleting associated dataset id from KDP. See printed error message above.\")\n",
    "            raise Exception('Ingest failed, exiting')\n",
    "\n",
    "    return new_dataset_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "a9700354",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_to_existing_kdp(data, target_dataset_id, workspace_id, jwt, batch_size, starting_record_id, ingestCheck, equivalenceCheck, similarCheck, returnNewData):\n",
    "    \n",
    "    '''\n",
    "    This function provides a way to more directly write into an existing KDP dataset and append additional rows\n",
    "    with Pandas. Since this can be potentially dangerous, there are safety checks in place to ensure the integrity\n",
    "    of the data.\n",
    "    \n",
    "    Parameters:\n",
    "    \n",
    "    data - pandas df to write to KDP, the one that was read and normalized/transformed.\n",
    "    dataset_id - the dataset ID of this of the dataset originally read into Jupyter\n",
    "    workspace_id - matches initial settings\n",
    "    jwt - matches initial settings\n",
    "    batch_size - matches initial settings\n",
    "    starting_record_id - matches initial settings\n",
    "    ingestCheck - boolean option to check if ingest is working properly.\n",
    "    equivalenceCheck - boolean option to check if input dataframe and reading from KDP are equivalent (having\n",
    "    the same rows or columns except in a different order). ingestCheck must be enabled for this check to work.\n",
    "    similarCheck - boolean option to check if the incoming data is similar to the target dataset. Similar\n",
    "    meaning same column names with the same data types.\n",
    "    returnNewData - boolean option to return the dataset with the appended data as output\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    ingestFailed = False\n",
    "       \n",
    "    #Check if ingest works\n",
    "    if ingestCheck:\n",
    "        #Create new dataset with temporary name\n",
    "        tempName = 'Temporary_'+ str(np.random.randint(10000000))\n",
    "        temp_dataset = kdp_conn.create_dataset(name=tempName, workspace_id=workspace_id, jwt=jwt)\n",
    "        temp_dataset_id = temp_dataset.id\n",
    "\n",
    "        #Ingest data into newly created dataset\n",
    "        try:\n",
    "            partitions_set = kdp_conn.ingest(data, temp_dataset_id, jwt, batch_size)\n",
    "        except:\n",
    "            ingestFailed = True\n",
    "     \n",
    "        if not ingestFailed:\n",
    "\n",
    "            #If ingest successful and perform equivalenceCheck\n",
    "            if equivalenceCheck:\n",
    "\n",
    "                dfCheck = kdp_conn.read_dataset_to_pandas_dataframe(dataset_id=temp_dataset_id,\n",
    "                                                      jwt=jwt,\n",
    "                                                      starting_record_id=starting_record_id,\n",
    "                                                      batch_size=batch_size)\n",
    "                if dfs_equivalent(dfCheck, data):\n",
    "                    print('equivalenceCheck pass')\n",
    "                else:\n",
    "                    print('equivalenceCheck fail:\\n')\n",
    "                    print('Input data length: {}\\n'.format(len(data)))\n",
    "                    print('Input data non-duplicated length: {}\\n'.format(len(data.drop_duplicates())))\n",
    "                    print('KDP data length: {}\\n'.format(len(dfCheck)))\n",
    "                    print('KDP data non-duplicated length: {}\\n'.format(len(dfCheck.drop_duplicates)))\n",
    "                    print('Merge data length: {}'.format(len(pd.merge(data.drop_duplicates(), dfCheck.drop_duplicates(), \n",
    "                                                                      on = list(data.columns), how = 'inner'))))\n",
    "                    raise Exception('equivalenceCheck failed, if wish to continue, disable this check.')\n",
    "\n",
    "\n",
    "    if similarCheck:\n",
    "        #Check if data similar to target dataset data\n",
    "        targetDf = kdp_conn.read_dataset_to_pandas_dataframe(dataset_id=target_dataset_id,\n",
    "                                                  jwt=jwt,\n",
    "                                                  starting_record_id=starting_record_id,\n",
    "                                                  batch_size=batch_size)\n",
    "\n",
    "        if dfs_similar_data(targetDf, data):\n",
    "            print('Similar data check pass')\n",
    "        else:\n",
    "            print('Similar data check failed. See below for differences.')\n",
    "            print(dfs_difference_types(targetDf, data))\n",
    "            raise Exception('Similar data check failed, if wish to continue, disable similarCheck.')\n",
    "\n",
    "    \n",
    "    if not ingestFailed:\n",
    "        #Ingest data into KDP dataset via append\n",
    "        try:\n",
    "            partitions_set = kdp_conn.ingest(data, target_dataset_id, jwt, batch_size)\n",
    "        except:\n",
    "            ingestFailed = True\n",
    "    \n",
    "    if ingestCheck:\n",
    "        \n",
    "        #API Config\n",
    "        configuration = kdp_api.Configuration(\n",
    "            host='https://api.dev.koverse.com'\n",
    "        )\n",
    "        configuration.access_token = jwt\n",
    "        \n",
    "        #Delete temporary dataset if ingestCheck enabled\n",
    "        with kdp_api.ApiClient(configuration) as api_client:\n",
    "            try:\n",
    "                print('Deleting temporary dataset')\n",
    "                api_instance = datasets_api.DatasetsApi(api_client)\n",
    "                api_instance.datasets_id_delete(id = temp_dataset_id)\n",
    "                print('Temporary dataset {} was successfully deleted.'.format(temp_dataset_id))\n",
    "            except kdp_api.ApiException as e:\n",
    "                print(\"Exception : %s\\n\" % e)\n",
    "                raise Exception(\"Error deleting associated dataset id from KDP. See printed error message above.\")\n",
    "                \n",
    "    if returnNewData:\n",
    "        targetDf = kdp_conn.read_dataset_to_pandas_dataframe(dataset_id=target_dataset_id,\n",
    "                                                  jwt=jwt,\n",
    "                                                  starting_record_id=starting_record_id,\n",
    "                                                  batch_size=batch_size)\n",
    "           \n",
    "        return targetDf\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15c40feb",
   "metadata": {},
   "source": [
    "# Starting from Jupyter (Having dataset of interest to write into KDP)\n",
    "\n",
    "# Manual Upload\n",
    "\n",
    "### When you want to manually read in data into KDP, it's very likely that you want to do one of two things.\n",
    "### 1. Create a new dataset and upload data into it.\n",
    "### 2. Append data into a similar existing dataset.\n",
    "\n",
    "-------------\n",
    "\n",
    "# 1. Create a new dataset and upload data into it.\n",
    "\n",
    "### First, initialize settings for KDP connector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "121c9f3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "################# REPLACE WITH YOUR INFO  #################\n",
    "email = 'spongebob@koverse.com'\n",
    "password = 'Password1!'\n",
    "path_to_ca_file = ''\n",
    "host = 'https://api.dev.koverse.com'\n",
    "workspace_id = 'spongebob'\n",
    "###########################################################\n",
    "\n",
    "#Connect\n",
    "kdp_conn = KdpConn(path_to_ca_file=path_to_ca_file, host=host)\n",
    "\n",
    "### passed from oauth env var in jupyterhub_config\n",
    "jwt = os.getenv('ACCESS_TOKEN')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b58fdf4",
   "metadata": {},
   "source": [
    "### Read in data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "778254ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('titanic.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dd30a61",
   "metadata": {},
   "source": [
    "### Some normalizations are required since writing to KDP currently requires no null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "726ab4a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize_titanic(data):\n",
    "    data['Age'] = round(data['Age'], 0).astype(str).apply(lambda x: x[:-2] if '.' in x else '')\n",
    "    data['Cabin'] = data['Cabin'].fillna('')\n",
    "    data['Embarked'] = data['Embarked'].fillna('')\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "ab874648",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = standardize_titanic(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "947998f6",
   "metadata": {},
   "source": [
    "### Use write_to_kdp function to write to a new dataset on KDP and output associated dataset ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "7805ed1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/danieljin/opt/anaconda3/lib/python3.9/site-packages/urllib3/connectionpool.py:1013: InsecureRequestWarning: Unverified HTTPS request is being made to host 'api.dev.koverse.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings\n",
      "  warnings.warn(\n",
      "/Users/danieljin/opt/anaconda3/lib/python3.9/site-packages/urllib3/connectionpool.py:1013: InsecureRequestWarning: Unverified HTTPS request is being made to host 'api.dev.koverse.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings\n",
      "  warnings.warn(\n",
      "/Users/danieljin/opt/anaconda3/lib/python3.9/site-packages/urllib3/connectionpool.py:1013: InsecureRequestWarning: Unverified HTTPS request is being made to host 'api.dev.koverse.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "equivalenceCheck pass\n"
     ]
    }
   ],
   "source": [
    "batch_size = 100000\n",
    "starting_record_id = ''\n",
    "\n",
    "\n",
    "#Use write_to_kdp function to write to new dataset on KDP and output associated dataset ID\n",
    "dataset_id = write_to_new_kdp(df, 'titanicTest', workspace_id, jwt, batch_size, starting_record_id, \n",
    "                          equivalenceCheck = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "810539bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c9f01fc9-33f9-48d7-a679-25e8ee30abe1'"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4957f765",
   "metadata": {},
   "source": [
    "# 2. Append data into a similar existing dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "8d8e4d16",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/danieljin/opt/anaconda3/lib/python3.9/site-packages/urllib3/connectionpool.py:1013: InsecureRequestWarning: Unverified HTTPS request is being made to host 'api.dev.koverse.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings\n",
      "  warnings.warn(\n",
      "/Users/danieljin/opt/anaconda3/lib/python3.9/site-packages/urllib3/connectionpool.py:1013: InsecureRequestWarning: Unverified HTTPS request is being made to host 'api.dev.koverse.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings\n",
      "  warnings.warn(\n",
      "/Users/danieljin/opt/anaconda3/lib/python3.9/site-packages/urllib3/connectionpool.py:1013: InsecureRequestWarning: Unverified HTTPS request is being made to host 'api.dev.koverse.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "equivalenceCheck pass\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/danieljin/opt/anaconda3/lib/python3.9/site-packages/urllib3/connectionpool.py:1013: InsecureRequestWarning: Unverified HTTPS request is being made to host 'api.dev.koverse.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similar data check pass\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/danieljin/opt/anaconda3/lib/python3.9/site-packages/urllib3/connectionpool.py:1013: InsecureRequestWarning: Unverified HTTPS request is being made to host 'api.dev.koverse.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleting temporary dataset\n",
      "Temporary dataset a23192f6-9181-4ab1-ab16-c4c3ca15f3dd was successfully deleted.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/danieljin/opt/anaconda3/lib/python3.9/site-packages/urllib3/connectionpool.py:1013: InsecureRequestWarning: Unverified HTTPS request is being made to host 'api.dev.koverse.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "df = write_to_existing_kdp(df, dataset_id, workspace_id, jwt, batch_size, starting_record_id, \n",
    "                      ingestCheck = True, equivalenceCheck = True, similarCheck = True, returnNewData = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "910d4bef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1782"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55f600db",
   "metadata": {},
   "source": [
    "# Reading from KDP\n",
    "\n",
    "### When you want to read something from KDP, it's very likely that you want to do one of two things.\n",
    "\n",
    "### 1. Transform a dataset and output the results into a new dataset.\n",
    "### 2. Normalize a dataset and overwrite the dataset, effectively deleting the old one.\n",
    "\n",
    "-----------\n",
    "# 1. Transform a dataset and output the results into a new dataset.\n",
    "\n",
    "### Initialize the connector the same as before then grab the dataset id off the URL in KDP for the dataset of interest and copy paste it into the variable below (sake of simplicity using same example dataset ID from above)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f8bed24",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset_id = ''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0506c83",
   "metadata": {},
   "source": [
    "### Read KDP dataset into a pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "7ecb2c1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/danieljin/opt/anaconda3/lib/python3.9/site-packages/urllib3/connectionpool.py:1013: InsecureRequestWarning: Unverified HTTPS request is being made to host 'api.dev.koverse.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "df = kdp_conn.read_dataset_to_pandas_dataframe(dataset_id=dataset_id,\n",
    "                                                  jwt=jwt,\n",
    "                                                  starting_record_id=starting_record_id,\n",
    "                                                  batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7709e12",
   "metadata": {},
   "source": [
    "### Perform desired transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "cb6e5fed",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['AnySibSp'] = df['SibSp'].apply(lambda x: 1 if x >= 1 else 0)\n",
    "df['AnyParch'] = df['Parch'].apply(lambda x: 1 if x >= 1 else 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "832f1df8",
   "metadata": {},
   "source": [
    "### Output results into new dataset on KDP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "9a25ad9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/danieljin/opt/anaconda3/lib/python3.9/site-packages/urllib3/connectionpool.py:1013: InsecureRequestWarning: Unverified HTTPS request is being made to host 'api.dev.koverse.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings\n",
      "  warnings.warn(\n",
      "/Users/danieljin/opt/anaconda3/lib/python3.9/site-packages/urllib3/connectionpool.py:1013: InsecureRequestWarning: Unverified HTTPS request is being made to host 'api.dev.koverse.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings\n",
      "  warnings.warn(\n",
      "/Users/danieljin/opt/anaconda3/lib/python3.9/site-packages/urllib3/connectionpool.py:1013: InsecureRequestWarning: Unverified HTTPS request is being made to host 'api.dev.koverse.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "equivalenceCheck pass\n"
     ]
    }
   ],
   "source": [
    "dataset_id = write_to_new_kdp(df, 'titanicTest2', workspace_id, jwt, batch_size, starting_record_id, \n",
    "                          equivalenceCheck = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67f437f6",
   "metadata": {},
   "source": [
    "### The new dataset ID is assigned, so this allows you to easily access and remember the results of this transform. This would be useful if more than one output is created from one or more datasets which are used as inputs in a future step. For those cases it may be worth appending the dataset_id into a list."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ddf7f96",
   "metadata": {},
   "source": [
    "# 2. Normalize a dataset and overwrite the dataset, effectively deleting the old one."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d54ca3cc",
   "metadata": {},
   "source": [
    "### Get dataset ID from KDP and read dataset into a pandas dataframe (For the sake of example using the same dataset ID from above. Under normal circumstances we probably wouldn't normalize and overwrite anything except the initial dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "3fd18868",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/danieljin/opt/anaconda3/lib/python3.9/site-packages/urllib3/connectionpool.py:1013: InsecureRequestWarning: Unverified HTTPS request is being made to host 'api.dev.koverse.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "df = kdp_conn.read_dataset_to_pandas_dataframe(dataset_id=dataset_id,\n",
    "                                                  jwt=jwt,\n",
    "                                                  starting_record_id=starting_record_id,\n",
    "                                                  batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "f041abb3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Embarked</th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Ticket</th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Sex</th>\n",
       "      <th>AnySibSp</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Name</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Parch</th>\n",
       "      <th>AnyParch</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Age</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>S</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>19943</td>\n",
       "      <td>487</td>\n",
       "      <td>female</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Hoyt, Mrs. Frederick Maxfield (Jane Anne Forby)</td>\n",
       "      <td>90.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>C93</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>S</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>113781</td>\n",
       "      <td>306</td>\n",
       "      <td>male</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Allison, Master. Hudson Trevor</td>\n",
       "      <td>151.6</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>C22 C26</td>\n",
       "      <td>0.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>S</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Fa 265302</td>\n",
       "      <td>155</td>\n",
       "      <td>male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Olsen, Mr. Ole Martin</td>\n",
       "      <td>7.3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>C</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>11767</td>\n",
       "      <td>311</td>\n",
       "      <td>female</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Hays, Miss. Margaret Bechstein</td>\n",
       "      <td>83.2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>C54</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>S</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>28228</td>\n",
       "      <td>419</td>\n",
       "      <td>male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Matthews, Mr. William John</td>\n",
       "      <td>13.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Embarked  Survived  Pclass     Ticket  PassengerId     Sex  AnySibSp  SibSp  \\\n",
       "0        S         1       1      19943          487  female         1      1   \n",
       "1        S         1       1     113781          306    male         1      1   \n",
       "2        S         0       3  Fa 265302          155    male         0      0   \n",
       "3        C         1       1      11767          311  female         0      0   \n",
       "4        S         0       2      28228          419    male         0      0   \n",
       "\n",
       "                                              Name   Fare  Parch  AnyParch  \\\n",
       "0  Hoyt, Mrs. Frederick Maxfield (Jane Anne Forby)   90.0      0         0   \n",
       "1                   Allison, Master. Hudson Trevor  151.6      2         1   \n",
       "2                            Olsen, Mr. Ole Martin    7.3      0         0   \n",
       "3                   Hays, Miss. Margaret Bechstein   83.2      0         0   \n",
       "4                       Matthews, Mr. William John   13.0      0         0   \n",
       "\n",
       "     Cabin Age  \n",
       "0      C93  35  \n",
       "1  C22 C26  0.  \n",
       "2               \n",
       "3      C54  24  \n",
       "4           30  "
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "4bbdeeee",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Fare'] = round(df['Fare'], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "97accf96",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/danieljin/opt/anaconda3/lib/python3.9/site-packages/urllib3/connectionpool.py:1013: InsecureRequestWarning: Unverified HTTPS request is being made to host 'api.dev.koverse.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings\n",
      "  warnings.warn(\n",
      "/Users/danieljin/opt/anaconda3/lib/python3.9/site-packages/urllib3/connectionpool.py:1013: InsecureRequestWarning: Unverified HTTPS request is being made to host 'api.dev.koverse.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings\n",
      "  warnings.warn(\n",
      "/Users/danieljin/opt/anaconda3/lib/python3.9/site-packages/urllib3/connectionpool.py:1013: InsecureRequestWarning: Unverified HTTPS request is being made to host 'api.dev.koverse.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings\n",
      "  warnings.warn(\n",
      "/Users/danieljin/opt/anaconda3/lib/python3.9/site-packages/urllib3/connectionpool.py:1013: InsecureRequestWarning: Unverified HTTPS request is being made to host 'api.dev.koverse.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "equivalenceCheck pass\n",
      "Ingest successful. Deleting old dataset.\n",
      "Dataset c9f01fc9-33f9-48d7-a679-25e8ee30abe1 was successfully deleted.\n"
     ]
    }
   ],
   "source": [
    "dataset_id = overwrite_to_kdp(df, dataset_id, workspace_id, jwt, batch_size, starting_record_id, \n",
    "                              equivalenceCheck = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fe1bb33",
   "metadata": {},
   "source": [
    "# Starting from Jupyter (Having dataset of interest to write into KDP)\n",
    "\n",
    "# Automatic Upload / External database / data pipeline setups\n",
    "\n",
    "### With real-time running processes, it's possible to do several different things depending on need. \n",
    "\n",
    "### With data being pulled every day, week, month, or other time interval, would define how frequently the read/write process would need to be run. \n",
    "\n",
    "### Here are just a few possible use cases of how something could be set up. No transformations will be used here for simplicity, though they would be used as necessary in reality.\n",
    "\n",
    "# New data sets ++ - Adding a newer timestamped dataset if it's important to distinguish and separate something by week, month, year, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8af536e",
   "metadata": {},
   "source": [
    "### Assume that the automatic data pull is somehow set up and is being read into a dataframe. We'll just continue to use a manual upload process"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac80a98e",
   "metadata": {},
   "source": [
    "### Here the data pull could run once a month, so the data could be labeled \"Trains_Mar2022\" for March 2022, \"Trains_Apr2022\" for April 2022 etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "075a157d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/danieljin/opt/anaconda3/lib/python3.9/site-packages/urllib3/connectionpool.py:1013: InsecureRequestWarning: Unverified HTTPS request is being made to host 'api.dev.koverse.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings\n",
      "  warnings.warn(\n",
      "/Users/danieljin/opt/anaconda3/lib/python3.9/site-packages/urllib3/connectionpool.py:1013: InsecureRequestWarning: Unverified HTTPS request is being made to host 'api.dev.koverse.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings\n",
      "  warnings.warn(\n",
      "/Users/danieljin/opt/anaconda3/lib/python3.9/site-packages/urllib3/connectionpool.py:1013: InsecureRequestWarning: Unverified HTTPS request is being made to host 'api.dev.koverse.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "equivalenceCheck pass\n"
     ]
    }
   ],
   "source": [
    "### Some automatic data pull process would run 1x month\n",
    "df = pd.read_csv('titanic.csv')\n",
    "df = standardize_titanic(df)\n",
    "datasetDate = datetime.today().strftime('%b-%Y')\n",
    "\n",
    "dataset_id = write_to_new_kdp(df, 'titanicTest_{}'.format(datasetDate), workspace_id, jwt, batch_size, starting_record_id, \n",
    "                          equivalenceCheck = True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8fd1af9",
   "metadata": {},
   "source": [
    "### It could also be a good idea to include an analytics summary report of each month as a separate transform of aggregations or custom report as another dataset. Comparisons could be done as per report values from previous month to the new month etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ebe9730",
   "metadata": {},
   "source": [
    "# Append ++ - Similar to above, except separating into different datasets is not important and use one dataset instead"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9af1768",
   "metadata": {},
   "source": [
    "### Create initial dataset since it's required to start appending. So, for the first month it would be a manual read."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "944f5b66",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/danieljin/opt/anaconda3/lib/python3.9/site-packages/urllib3/connectionpool.py:1013: InsecureRequestWarning: Unverified HTTPS request is being made to host 'api.dev.koverse.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings\n",
      "  warnings.warn(\n",
      "/Users/danieljin/opt/anaconda3/lib/python3.9/site-packages/urllib3/connectionpool.py:1013: InsecureRequestWarning: Unverified HTTPS request is being made to host 'api.dev.koverse.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings\n",
      "  warnings.warn(\n",
      "/Users/danieljin/opt/anaconda3/lib/python3.9/site-packages/urllib3/connectionpool.py:1013: InsecureRequestWarning: Unverified HTTPS request is being made to host 'api.dev.koverse.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "equivalenceCheck pass\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('titanic.csv')\n",
    "df = standardize_titanic(df)\n",
    "datasetDate = datetime.today().strftime('%b-%Y')\n",
    "\n",
    "#Create datasetDate column to track time. (Optional if desired to track)\n",
    "df['DatasetDate'] = datasetDate\n",
    "\n",
    "#Write to KDP\n",
    "dataset_id = write_to_new_kdp(df, 'titanicAppendTest', workspace_id, jwt, batch_size, starting_record_id, \n",
    "                          equivalenceCheck = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa6977aa",
   "metadata": {},
   "source": [
    "### Assume that the automatic data pull is somehow set up and is being read into a dataframe. We'll just continue to use a manual upload process"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cea4982",
   "metadata": {},
   "source": [
    "### Assume data pull runs once a month again and want to track the date of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "c08ebf71",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/danieljin/opt/anaconda3/lib/python3.9/site-packages/urllib3/connectionpool.py:1013: InsecureRequestWarning: Unverified HTTPS request is being made to host 'api.dev.koverse.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings\n",
      "  warnings.warn(\n",
      "/Users/danieljin/opt/anaconda3/lib/python3.9/site-packages/urllib3/connectionpool.py:1013: InsecureRequestWarning: Unverified HTTPS request is being made to host 'api.dev.koverse.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings\n",
      "  warnings.warn(\n",
      "/Users/danieljin/opt/anaconda3/lib/python3.9/site-packages/urllib3/connectionpool.py:1013: InsecureRequestWarning: Unverified HTTPS request is being made to host 'api.dev.koverse.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "equivalenceCheck pass\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/danieljin/opt/anaconda3/lib/python3.9/site-packages/urllib3/connectionpool.py:1013: InsecureRequestWarning: Unverified HTTPS request is being made to host 'api.dev.koverse.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similar data check pass\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/danieljin/opt/anaconda3/lib/python3.9/site-packages/urllib3/connectionpool.py:1013: InsecureRequestWarning: Unverified HTTPS request is being made to host 'api.dev.koverse.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleting temporary dataset\n",
      "Temporary dataset c4068f51-f4fb-4703-96e0-c2f6f15fc8c1 was successfully deleted.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/danieljin/opt/anaconda3/lib/python3.9/site-packages/urllib3/connectionpool.py:1013: InsecureRequestWarning: Unverified HTTPS request is being made to host 'api.dev.koverse.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('titanic.csv')\n",
    "df = standardize_titanic(df)\n",
    "\n",
    "datasetDate = (datetime.today() + relativedelta(months = 1)).strftime('%b-%Y')\n",
    "\n",
    "#Create datasetDate column to track time. (Optional if desired to track, required if previously used)\n",
    "df['DatasetDate'] = datasetDate\n",
    "\n",
    "df = write_to_existing_kdp(df, dataset_id, workspace_id, jwt, batch_size, starting_record_id, \n",
    "                      ingestCheck = True, equivalenceCheck = True, similarCheck = True, returnNewData = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7560dc81",
   "metadata": {},
   "source": [
    "### Since it uses the same dataset_id and the variable may not be saved forever, it's ideal to save the existing dataset_id directly into a script that would perform this process or into a text file."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c93d0205",
   "metadata": {},
   "source": [
    "# Replace ++ - Newer versions of the same datasets would directly replace the existing dataset. May be most useful in cases when reference files/datasets need to be periodically updated"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f8ba6bf",
   "metadata": {},
   "source": [
    "### Create initial dataset since it's required to start replacing. So, for the first month it would be a manual read."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "e68e6c7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/danieljin/opt/anaconda3/lib/python3.9/site-packages/urllib3/connectionpool.py:1013: InsecureRequestWarning: Unverified HTTPS request is being made to host 'api.dev.koverse.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings\n",
      "  warnings.warn(\n",
      "/Users/danieljin/opt/anaconda3/lib/python3.9/site-packages/urllib3/connectionpool.py:1013: InsecureRequestWarning: Unverified HTTPS request is being made to host 'api.dev.koverse.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings\n",
      "  warnings.warn(\n",
      "/Users/danieljin/opt/anaconda3/lib/python3.9/site-packages/urllib3/connectionpool.py:1013: InsecureRequestWarning: Unverified HTTPS request is being made to host 'api.dev.koverse.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "equivalenceCheck pass\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('titanic.csv')\n",
    "df = standardize_titanic(df)\n",
    "\n",
    "#Write to KDP\n",
    "dataset_id = write_to_new_kdp(df, 'titanicReplaceTest', workspace_id, jwt, batch_size, starting_record_id, \n",
    "                          equivalenceCheck = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6b8f0ff",
   "metadata": {},
   "source": [
    "### Assume that the automatic data pull is somehow set up and is being read into a dataframe. We'll just continue to use a manual upload process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6688337",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('titanic.csv')\n",
    "df = standardize_titanic(df)\n",
    "\n",
    "#Can read in last data pull, or current version of the data\n",
    "df2 = kdp_conn.read_dataset_to_pandas_dataframe(dataset_id=dataset_id,\n",
    "                                                  jwt=jwt,\n",
    "                                                  starting_record_id=starting_record_id,\n",
    "                                                  batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c458a555",
   "metadata": {},
   "source": [
    "### Now could potentially compare the current version with the new version and directly find the differences between each other or create aggregated/custom reports to find high level differences etc. This could go into a new dataset which could track all the differences between all the datapulls and be part of an Append++ flow series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "79fe5b90",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/danieljin/opt/anaconda3/lib/python3.9/site-packages/urllib3/connectionpool.py:1013: InsecureRequestWarning: Unverified HTTPS request is being made to host 'api.dev.koverse.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings\n",
      "  warnings.warn(\n",
      "/Users/danieljin/opt/anaconda3/lib/python3.9/site-packages/urllib3/connectionpool.py:1013: InsecureRequestWarning: Unverified HTTPS request is being made to host 'api.dev.koverse.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings\n",
      "  warnings.warn(\n",
      "/Users/danieljin/opt/anaconda3/lib/python3.9/site-packages/urllib3/connectionpool.py:1013: InsecureRequestWarning: Unverified HTTPS request is being made to host 'api.dev.koverse.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings\n",
      "  warnings.warn(\n",
      "/Users/danieljin/opt/anaconda3/lib/python3.9/site-packages/urllib3/connectionpool.py:1013: InsecureRequestWarning: Unverified HTTPS request is being made to host 'api.dev.koverse.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings\n",
      "  warnings.warn(\n",
      "/Users/danieljin/opt/anaconda3/lib/python3.9/site-packages/urllib3/connectionpool.py:1013: InsecureRequestWarning: Unverified HTTPS request is being made to host 'api.dev.koverse.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "equivalenceCheck pass\n",
      "Ingest successful. Deleting old dataset.\n",
      "Dataset 96446bfa-8fa9-4c8e-965d-e25c7be93bdc was successfully deleted.\n"
     ]
    }
   ],
   "source": [
    "dataset_id = overwrite_to_kdp(df, dataset_id, workspace_id, jwt, batch_size, starting_record_id, \n",
    "                              equivalenceCheck = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6d1e949",
   "metadata": {},
   "source": [
    "### Since the dataset_id changes with every overwrite in the current implementation of KDP4, it would be good to save the current dataset_id directly into a text file so that a script can read the dataset id directly off the text file and write into the file in cases when the script stops and loses track of the variable.\n",
    "\n",
    "### An alternative is to have an Excel sheet to keep track of which datasets/files are using which dataset ID, which processes (Append++, Replace++, New data++), sources, etc so everything is contained in one centralized file, then for the case of Replace++, the current dataset ID can be overwritten in a specific cell in that file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d235b68",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c6c3735",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f263e6da",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1c6675e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "293ce4c5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7fc2e52",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "178aebd4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bce07dd3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d34a54b7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
